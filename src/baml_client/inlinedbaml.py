# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {
    "caller.baml": '// BAML configuration for intent and question extraction\n\nclient<llm> Gemini {\n  provider vertex-ai\n    options {\n    model gemini-2.5-flash\n    project_id env.PROJECT_ID\n    location env.GEMINI_REGION\n    // base_url "https://aiplatform.googleapis.com/v1/projects/grotto-prod/locations/global/publishers/google/models"\n    generation_config {\n      temperature 0.0\n      seed 42\n      top_p 0.95\n      thinkingConfig {\n          thinkingBudget 0\n    }\n    }\n  }\n}\n\n\n// Extract questions from the caller\nfunction ExtractQuestions(conversation: string) -> string[] {\n  client Gemini\n  prompt #"\n    Given the following conversation from a car rental inquiry call,\n    extract all the distinct questions the caller has asked.\n    Return only the questions, one per line.\n    \n    Conversation: {{ conversation }}\n    \n    Questions:\n  "#\n}\n\n// Extract renter profile information\nclass CallerProfile {\n  name string? @description("Name of the caller if mentioned")\n  phone string? @description("Phone number if mentioned")\n  email string? @description("Email address if mentioned")\n  rental_dates string? @description("Desired rental dates if mentioned")\n  car_preferences string[] @description("Car type or feature preferences mentioned")\n  budget_low float?\n  budget_high float?\n  location string? @description("Pickup/dropoff location if mentioned")\n  additional_notes string[] @description("Any other relevant information")\n  intent ("buying" | "renting" | "inquiry" | "other")? @description("Primary intent of the caller")\n}\n\nclass CallerData {\n  profile CallerProfile\n  questions string[]\n}\n\nfunction ExtractRenterProfile(conversation: string) -> CallerProfile {\n  client Gemini\n  prompt #"\n    Extract renter profile information from this car rental inquiry conversation.\n    Only include information that was explicitly mentioned. Use null for missing fields.\n    \n    Conversation: {{ conversation }}\n    \n    {{ ctx.output_format }}\n  "#\n}\n',
    "cars.baml": 'type CarType = ("sedan" | "suv" | "truck" | "van" | "coupe" | "convertible" | "hatchback" | "wagon" | "sports"| "electric" | "luxury" | "other")\ntype SaleType = ("rental" | "sale" | "both")\ntype PriceUnit = ("usd_per_day" | "usd")\n\nclass CarInfo {\n  make string\n  model string\n  year int\n  type CarType\n  sale_type SaleType\n  price int          @description("For sale price OR rental price per day")\n  price_unit PriceUnit\n  fuel_efficiency int\n  features string[]\n  horsepower int\n  seats int\n}',
    "generators.baml": '// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: "python/pydantic", "typescript", "ruby/sorbet", "rest/openapi"\n    output_type "python/pydantic"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir "../src"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version "0.214.0"\n\n    // Valid values: "sync", "async"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n',
    "resume.baml": '// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // You can also use custom LLM params with a custom client name from clients.baml like "client CustomGPT5" or "client CustomSonnet4"\n  client "openai-responses/gpt-5-mini" // Set OPENAI_API_KEY to use this client.\n  prompt #"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  "#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    "#\n  }\n}\n',
}


def get_baml_files():
    return _file_map
